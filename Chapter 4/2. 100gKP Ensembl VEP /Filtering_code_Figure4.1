#In order to filter variants as shown in Chapter 4, Figure 4.1. I used a general template and interchanged the basic code for each task.

#As shown below, for the 100kGP double helix super computer I used:

#BSUB -q short  # depending on the number of files that you will need
#BSUB -P re_gecip_cancer_colorectal # edit this line
#BSUB -o %J.stdout
#BSUB -e %J.stderr
#BSUB -J "duplicateposcolumn[1-143]%143"
#BSUB -cwd .
#BSUB -n 1 # this will need to be the same as the number of VEP forks
#BSUB -R "rusage[mem=64000] span[hosts=1]"
#BSUB -M 64000

IDs_file='/file/containing/the/list/of/file/IDs/you/want/toextract/'

CHROM_PASS=/file/containing/words/you/want/to/extract/or/input/the/name/of/.txt
FILE_INPUT_PATH=/path/to/the/large/file/that/contains/all/of/the/annotated/patient/variants/
FILE_INPUT_FILE=/large/file/that/contains/all/of/the/annotated/patient/variants/.txt

PASS_ONLY_OUTPUT_FILEPATH=/path/to/the/output/file/that/contains/only/the/annotated/patient/variants/that/PASS/variantcalling/
PASS_ONLY_OUTPUT=/file/that/contains/only/the/annotated/patient/variants/that/PASS/variantcalling/.txt

FILE_ID=$(sed -n "${LSB_JOBINDEX}p" "$IDs_file")

LC_ALL-C grep -wf $CHROM_HIGH_MOD "${FILE_INPUT_PATH}${FILE_ID}${FILE_INPUT_FILE}" > "${PASS_ONLY_OUTPUT_FILEPATH}${FILE_ID}${PASS_ONLY_OUTPUT}"

#For each filtering option that included a word, I changed the CHROM_PASS file name and contents to match; for example: to filter for only HIGH and MODERATE impact variants I made a file called CHROM_HIGH_MODERATE, which contained:

CHROM
HIGH 
MODERATE

#Which can then be used as the input file for grep -wf. 

#The template for submitting to the supercomputer stayed the same with alternative lines of codes used for the different filtering steps using.

//////////////////////////////////////////////////////////////////////////////////////////////////

The following occur in order, so the output of the previous step should be the input of the following step.

# 1. PASS only
#$1 is the column number for the FILTER_PASS.txt list, so column number 1.
#$7 is the column number in the combined annotated variant file where the Filter column is, so column 7.

#Filter_PASS.txt is a text file with a list of:

Filter
PASS

#This is to ensure only the header and variants assigned as PASS are returned.
$7

awk 'FNR==NR{a[$1]++;next}a[$7]' Filter_PASS.txt combined_annotated_variants.txt > PASSonly_output.txt

# 2. HIGH and MODERATE impact only
#$1 is the column number for the IMPACT_HIGH_MODERATE.txt list, so column number 1.
#$10 is the column number in the combined annotated variant file where the IMPACT column is, so column 10.

#IMPACT_HIGH_MODERATE.txt is a text file with a list of:

IMPACT
HIGH
MODERATE

#This is to ensure only the header and variants assigned as HIGH or MODERATE impact are returned.
awk 'FNR==NR{a[$1]++;next}a[$10]' IMPACT_HIGH_MODERATE.txt PASSonly_output.txt > HIGHMODonly_output.txt

#3. MANE only
#Extract the column which has the MANE_SELECT (column 33). Using:
awk '{print $33}' HIGHMODonly_output.txt > MANE_SELECT_column.txt
#Then sort using 
sort MANE_SELECT_column.txt > sort_MANE_SELECT_column.txt
#Then remove duplicates using
uniq sort_MANE_SELECT_column.txt > uniq_MANE_SELECT_column.txt 
#Open file and delete the row with a "." and save file.
#Next use 
awk 'FNR==NR{a[$1]++;next}a[$33]' uniq_MANE_SELECT_column.txt HIGHMODonly_output.txt > MANEonly_output.txt

#4. Keep MAF equal or less than 0.01 in the 1000 Genomes Project column (column 49).
awk 'NR ==1 { print } NR != 1 && $49 <= 0.01 {print}' MANEonly_output.txt > 1000genomesproject_MAF_0.01.txt

#5. Keep MAF equal or less than 0.01 in the GnomAD exomes (column 55).
awk 'NR ==1 { print } NR != 1 && $55 <= 0.01 {print}' 1000genomesproject_MAF_0.01.txt > GnomAD_exomes_MAF_0.01.txt

#6. Keep MAF equal or less than 0.01 in the GnomAD genomes (column 64).
awk 'NR ==1 { print } NR != 1 && $64 <= 0.01 {print}' GnomAD_exomes_MAF_0.01.txt > GnomAD_genomes_MAF_0.01.txt

#7. #Extract from the SIFT column (column 44) entries that contain the word deleterious.
awk 'NR==1 { print } NR != 1 && $44 ~ /deleterious/ {print}' GnomAD_genomes_MAF_0.01.txt > SIFT_deleterious.txt
#Then add extract entries from the SIFT columm that are "."
awk 'NR==1 { print } NR != 1 && $44=="." {print}' SIFT_deleterious.txt > SIFT_dots.txt
#Then combined the two files
cat SIFT_deleterious.txt SIFT_dots.txt >> SIFT_deleterious_dots.txt

#8. #Extract from the PolyPhen column (column 45) entries that contain the word damaging.
awk 'NR==1 { print } NR != 1 && $45 ~ /damaging/ {print}' SIFT_deleterious_dots.txt > PolyPhen_damaging.txt
#Then add extract entries from the PolyPhen columm that are "."
awk 'NR==1 { print } NR != 1 && $45=="." {print}' SIFT_deleterious_dots.txt > PolyPhen_dots.txt
#Then add extract entries from the PolyPhen column that are "unknown"
awk 'NR==1 { print } NR != 1 && $45 ~ /damaging/ {print}' SIFT_deleterious_dots.txt > PolyPhen_unknown.txt
#Then combined the three files
cat PolyPhen_damaging.txt PolyPhen_dots.txt PolyPhen_unknown.txt >> PolyPhen_damaging_dots_unknown.txt

#9. #Extract all missense variants from the Consequence column (column 9) in PolyPhen_damaging_dots_unknown.txt
awk 'NR==1 { print } NR != 1 && $9=="missense_variant" {print} PolyPhen_damaging_dots_unknown.txt > missense_PolyPhen_damaging_dots_unknown.txt
#Extract all other entries from the Consequence column (column 9) in PolyPhen_damaging_dots_unknown.txt by doing 
awk '{print $9'} PolyPhen_damaging_dots_unknown.txt > column9_PolyPhen_damaging_dots_unknown.txt
sort column9_PolyPhen_damaging_dots_unknown.txt > sort_column9_PolyPhen_damaging_dots_unknown.txt
uniq column9_PolyPhen_damaging_dots_unknown.txt > uniq_column9_PolyPhen_damaging_dots_unknown.txt
#Then remove entry with missense_variant from uniq_column9_PolyPhen_damaging_dots_unknown.txt and save file.
#Then keep only the variant entries without the missense_variant consequence
awk 'FNR==NR{a[$1]++;next}a[$9]' uniq_column9_PolyPhen_damaging_dots_unknown.txt PolyPhen_damaging_dots_unknown.txt > Non_missense_variant_consequence_PolyPhen_damaging_dots_unknown.txt

#Then extract only entries with a CADD score (column 86) equal or more than 24.2.
awk 'NR ==1 { print } NR != 1 && $89 >= 24.2 {print}' missense_PolyPhen_damaging_dots_unknown.txt > CADD24.2_missense_PolyPhen_damaging_dots_unknown.txt
Then combined CADD24.2_missense_PolyPhen_damaging_dots_unknown.txt and Non_missense_variant_consequence_PolyPhen_damaging_dots_unknown.txt.
cat CADD24.2_missense_PolyPhen_damaging_dots_unknown.txt Non_missense_variant_consequence_PolyPhen_damaging_dots_unknown.txt > CADD24.2.txt

#10. #Remove inframe indels and inframe indel splice 
awk '{print $9}' CADD24.2.txt > column9_CADD24.2.txt
sort column9_CADD24.2.txt > sort_column9_CADD24.2.txt
uniq sort_column9_CADD24.2.txt > uniq_column9_CADD24.2.txt
#Then remove entries from uniq_column9_CADD24.2.txt that are inframe indels or inframe indel splice and save the file.
#Now extract only entries with consequences in uniq_CADD24.2.txt from CADD24.2.txt
awk 'FNR==NR{a[$1]++;next}a[$9]' column9_CADD24.2.txt CADD24.2.txt > noinframeindels_inframeindelsplice.txt

$11. #Now filtering out genotypes with 0,0 using the same method as above (column 88 is genotype).
awk '{print $88}' noinframeindels_inframeindelsplice.txt > column88_noinframeindels_inframeindelsplice.txt
sort column88_noinframeindels_inframeindelsplice.txt > sort_column88_noinframeindels_inframeindelsplice.txt
uniq sort_column88_noinframeindels_inframeindelsplice.txt > uniq_column88_noinframeindels_inframeindelsplice.txt
#Then open uniq_column88_noinframeindels_inframeindelsplice.txt and delete the entry for 0,0 and save the file.

#Then extract all non-0,0 entries using
awk 'FNR==NR{a[$1]++;next}a[$88]' uniq_column88_noinframeindels_inframeindelsplice.txt noinframeindels_inframeindelsplice.txt > no0_0genotype.txt

$12. #Manaually open the text files in excel and calculate the variant cohort allele frequency for each variant in the cohort. Then filter out variants which have a variant cohort allele frequency greater than 0.121. Then save this as a tab delimited file again.

#13. #Manually opened the tab delimited file in excel and remove variants with a read depth less than 6. 





















